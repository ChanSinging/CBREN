Namespace(batchSize=16, cuda=True, dataset='datasets/', device='0', lr=0.0001, momentum=0.9, nEpochs=1000, resume='', start_epoch=1, step=1000, test_path='dataset/test/', threads=16, validate_path='datasets/validate/', weight_decay=0)
===> Loading datasets
===> Building model
n_pyramids: 1 n_pyramid_cells: (3, 2, 1, 1, 1, 1)
===> Setting GPU
===> Setting Optimizer
===> Training
Epoch=1, lr=0.0001
===> Epoch[1](1/3750): Loss: 0.4007631838
===> Epoch[1](2/3750): Loss: 0.3688431382
===> Epoch[1](3/3750): Loss: 0.3788600564
===> Epoch[1](4/3750): Loss: 0.2798349559
===> Epoch[1](5/3750): Loss: 0.3634732664
===> Epoch[1](6/3750): Loss: 0.4014689028
===> Epoch[1](7/3750): Loss: 0.3619157970
===> Epoch[1](8/3750): Loss: 0.3763625324
===> Epoch[1](9/3750): Loss: 0.3601952195
===> Epoch[1](10/3750): Loss: 0.3956667483
===> Epoch[1](11/3750): Loss: 0.4045538902
===> Epoch[1](12/3750): Loss: 0.3764842749
===> Epoch[1](13/3750): Loss: 0.3432312012
===> Epoch[1](14/3750): Loss: 0.2769711614
===> Epoch[1](15/3750): Loss: 0.2604765296
===> Epoch[1](16/3750): Loss: 0.3522268832
===> Epoch[1](17/3750): Loss: 0.3740124702
===> Epoch[1](18/3750): Loss: 0.3587202728
===> Epoch[1](19/3750): Loss: 0.2957873940
===> Epoch[1](20/3750): Loss: 0.3392411768
===> Epoch[1](21/3750): Loss: 0.2903149128
===> Epoch[1](22/3750): Loss: 0.2294173092
===> Epoch[1](23/3750): Loss: 0.2642270625
===> Epoch[1](24/3750): Loss: 0.2559259832
===> Epoch[1](25/3750): Loss: 0.2028563321
===> Epoch[1](26/3750): Loss: 0.2197062075
===> Epoch[1](27/3750): Loss: 0.1953312606
===> Epoch[1](28/3750): Loss: 0.1967655122
===> Epoch[1](29/3750): Loss: 0.1784558445
===> Epoch[1](30/3750): Loss: 0.1428373605
===> Epoch[1](31/3750): Loss: 0.1493014544
===> Epoch[1](32/3750): Loss: 0.1265661418
===> Epoch[1](33/3750): Loss: 0.1100067496
===> Epoch[1](34/3750): Loss: 0.1197763756
===> Epoch[1](35/3750): Loss: 0.1009767577
===> Epoch[1](36/3750): Loss: 0.0953653827
===> Epoch[1](37/3750): Loss: 0.1102254987
===> Epoch[1](38/3750): Loss: 0.1117079780
===> Epoch[1](39/3750): Loss: 0.1165493429
===> Epoch[1](40/3750): Loss: 0.1121963486
===> Epoch[1](41/3750): Loss: 0.1179000437
===> Epoch[1](42/3750): Loss: 0.1187173948
===> Epoch[1](43/3750): Loss: 0.1120940968
===> Epoch[1](44/3750): Loss: 0.1365724653
===> Epoch[1](45/3750): Loss: 0.1375998706
===> Epoch[1](46/3750): Loss: 0.1228144169
===> Epoch[1](47/3750): Loss: 0.1366126537
===> Epoch[1](48/3750): Loss: 0.1148763448
===> Epoch[1](49/3750): Loss: 0.0997565091
===> Epoch[1](50/3750): Loss: 0.1003083140
===> Epoch[1](51/3750): Loss: 0.1151885986
===> Epoch[1](52/3750): Loss: 0.0956102163
===> Epoch[1](53/3750): Loss: 0.1072876826
===> Epoch[1](54/3750): Loss: 0.0938900486
===> Epoch[1](55/3750): Loss: 0.1066305190
===> Epoch[1](56/3750): Loss: 0.1072200537
===> Epoch[1](57/3750): Loss: 0.0991926938
===> Epoch[1](58/3750): Loss: 0.1147735640
===> Epoch[1](59/3750): Loss: 0.1082295552
===> Epoch[1](60/3750): Loss: 0.0793466941
===> Epoch[1](61/3750): Loss: 0.1052078754
===> Epoch[1](62/3750): Loss: 0.0915019140
===> Epoch[1](63/3750): Loss: 0.0958980471
===> Epoch[1](64/3750): Loss: 0.1038643718
===> Epoch[1](65/3750): Loss: 0.0891343951
===> Epoch[1](66/3750): Loss: 0.0960084051
===> Epoch[1](67/3750): Loss: 0.0924563035
===> Epoch[1](68/3750): Loss: 0.0748979449
===> Epoch[1](69/3750): Loss: 0.0990150422
===> Epoch[1](70/3750): Loss: 0.0837616250
===> Epoch[1](71/3750): Loss: 0.0884266943
===> Epoch[1](72/3750): Loss: 0.0799386501
===> Epoch[1](73/3750): Loss: 0.0774367228
===> Epoch[1](74/3750): Loss: 0.0949535295
===> Epoch[1](75/3750): Loss: 0.1072724313
===> Epoch[1](76/3750): Loss: 0.0665719435
===> Epoch[1](77/3750): Loss: 0.0841011107
===> Epoch[1](78/3750): Loss: 0.0998746008
===> Epoch[1](79/3750): Loss: 0.0726275593
===> Epoch[1](80/3750): Loss: 0.0913736597
===> Epoch[1](81/3750): Loss: 0.0772608742
===> Epoch[1](82/3750): Loss: 0.0850872844
===> Epoch[1](83/3750): Loss: 0.0976805016
===> Epoch[1](84/3750): Loss: 0.0843715146
===> Epoch[1](85/3750): Loss: 0.0963164121
===> Epoch[1](86/3750): Loss: 0.0941357538
===> Epoch[1](87/3750): Loss: 0.0836905539
===> Epoch[1](88/3750): Loss: 0.0750768632
===> Epoch[1](89/3750): Loss: 0.0840397477
===> Epoch[1](90/3750): Loss: 0.0774148107
===> Epoch[1](91/3750): Loss: 0.0825701505
===> Epoch[1](92/3750): Loss: 0.0769175962
===> Epoch[1](93/3750): Loss: 0.0878741741
===> Epoch[1](94/3750): Loss: 0.0954684988
===> Epoch[1](95/3750): Loss: 0.0831838548
===> Epoch[1](96/3750): Loss: 0.0818410367
===> Epoch[1](97/3750): Loss: 0.0714341328
===> Epoch[1](98/3750): Loss: 0.0882083252
===> Epoch[1](99/3750): Loss: 0.0755938962
===> Epoch[1](100/3750): Loss: 0.0979068801
===> Epoch[1](101/3750): Loss: 0.0746047944
===> Epoch[1](102/3750): Loss: 0.0794568956
===> Epoch[1](103/3750): Loss: 0.0866527632
===> Epoch[1](104/3750): Loss: 0.1036717892
===> Epoch[1](105/3750): Loss: 0.1079091653
===> Epoch[1](106/3750): Loss: 0.0772612393
===> Epoch[1](107/3750): Loss: 0.0671546906
===> Epoch[1](108/3750): Loss: 0.0785969645
===> Epoch[1](109/3750): Loss: 0.0707729161
===> Epoch[1](110/3750): Loss: 0.0965424106
===> Epoch[1](111/3750): Loss: 0.1095347255
===> Epoch[1](112/3750): Loss: 0.1029877812
===> Epoch[1](113/3750): Loss: 0.0740387514
===> Epoch[1](114/3750): Loss: 0.0624710582
===> Epoch[1](115/3750): Loss: 0.0801153779
===> Epoch[1](116/3750): Loss: 0.0649894848
===> Epoch[1](117/3750): Loss: 0.0843940824
===> Epoch[1](118/3750): Loss: 0.0857058987
===> Epoch[1](119/3750): Loss: 0.0726479664
===> Epoch[1](120/3750): Loss: 0.0843783468
===> Epoch[1](121/3750): Loss: 0.0765129626
===> Epoch[1](122/3750): Loss: 0.0856457055
===> Epoch[1](123/3750): Loss: 0.0768343955
===> Epoch[1](124/3750): Loss: 0.0795312002
Traceback (most recent call last):
  File "train_psnr_indication.py", line 321, in <module>
    main()
  File "train_psnr_indication.py", line 164, in main
    train(training_data_loader, optimizer, model, criterion, epoch)
  File "train_psnr_indication.py", line 213, in train
    optimizer.step()
  File "/home/pxk/anaconda3/envs/cxy/lib/python3.7/site-packages/torch/optim/optimizer.py", line 89, in wrapper
    return func(*args, **kwargs)
  File "/home/pxk/anaconda3/envs/cxy/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/pxk/anaconda3/envs/cxy/lib/python3.7/site-packages/torch/optim/adam.py", line 119, in step
    group['eps'])
  File "/home/pxk/anaconda3/envs/cxy/lib/python3.7/site-packages/torch/optim/_functional.py", line 85, in adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
KeyboardInterrupt