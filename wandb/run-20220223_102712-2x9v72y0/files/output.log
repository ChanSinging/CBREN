Namespace(batchSize=16, cuda=True, dataset='datasets/', lr=0.0001, momentum=0.9, nEpochs=2500, resume='', start_epoch=1, step=1000, test_path='dataset/test/', threads=16, validate_path='datasets/validate/', weight_decay=0)
===> Loading datasets
===> Building model
n_pyramids: 1 n_pyramid_cells: (3, 2, 1, 1, 1, 1)
===> Setting GPU
===> Setting Optimizer
===> Training
Epoch=1, lr=0.0001
===> Epoch[1](1/11): Loss: 0.4475709796
===> Epoch[1](2/11): Loss: 0.3821965456
===> Epoch[1](3/11): Loss: 0.4474746585
===> Epoch[1](4/11): Loss: 0.3186744153
===> Epoch[1](5/11): Loss: 0.3878490329
===> Epoch[1](6/11): Loss: 0.3998662829
===> Epoch[1](7/11): Loss: 0.3505634069
===> Epoch[1](8/11): Loss: 0.3812944889
===> Epoch[1](9/11): Loss: 0.3440674245
===> Epoch[1](10/11): Loss: 0.2987433076
===> Epoch[1](11/11): Loss: 0.4293689430
|||||||||||||||||||||min_epoch_loss is 0.3806972260|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:1--Avg-PSNR: 6.2157 dB
-----max psnr model saved------
Epoch=2, lr=0.0001
===> Epoch[2](1/11): Loss: 0.4620667398
===> Epoch[2](2/11): Loss: 0.3366469145
===> Epoch[2](3/11): Loss: 0.3423142433
===> Epoch[2](4/11): Loss: 0.3937608600
===> Epoch[2](5/11): Loss: 0.3126374483
===> Epoch[2](6/11): Loss: 0.3771463037
===> Epoch[2](7/11): Loss: 0.3640427291
===> Epoch[2](8/11): Loss: 0.3574885726
===> Epoch[2](9/11): Loss: 0.3551455140
===> Epoch[2](10/11): Loss: 0.3350172043
===> Epoch[2](11/11): Loss: 0.3693306148
|||||||||||||||||||||min_epoch_loss is 0.3641451949|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:2--Avg-PSNR: 7.0986 dB
-----max psnr model saved------
Epoch=3, lr=0.0001
===> Epoch[3](1/11): Loss: 0.4222331047
===> Epoch[3](2/11): Loss: 0.3137645423
===> Epoch[3](3/11): Loss: 0.2925634980
===> Epoch[3](4/11): Loss: 0.2328151166
===> Epoch[3](5/11): Loss: 0.2855439186
===> Epoch[3](6/11): Loss: 0.2924647927
===> Epoch[3](7/11): Loss: 0.2535681427
===> Epoch[3](8/11): Loss: 0.2639412284
===> Epoch[3](9/11): Loss: 0.2417816371
===> Epoch[3](10/11): Loss: 0.2533991933
===> Epoch[3](11/11): Loss: 0.1850803196
|||||||||||||||||||||min_epoch_loss is 0.2761050449|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:3--Avg-PSNR: 12.6675 dB
-----max psnr model saved------
Epoch=4, lr=0.0001
===> Epoch[4](1/11): Loss: 0.2068988532
===> Epoch[4](2/11): Loss: 0.1749648154
===> Epoch[4](3/11): Loss: 0.1523721665
===> Epoch[4](4/11): Loss: 0.1181624681
===> Epoch[4](5/11): Loss: 0.2161529511
===> Epoch[4](6/11): Loss: 0.1160725951
===> Epoch[4](7/11): Loss: 0.1287234873
===> Epoch[4](8/11): Loss: 0.1267649084
===> Epoch[4](9/11): Loss: 0.1452281773
===> Epoch[4](10/11): Loss: 0.1160797179
===> Epoch[4](11/11): Loss: 0.1516277492
|||||||||||||||||||||min_epoch_loss is 0.1502770809|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:4--Avg-PSNR: 14.8682 dB
-----max psnr model saved------
Epoch=5, lr=0.0001
===> Epoch[5](1/11): Loss: 0.1356402636
===> Epoch[5](2/11): Loss: 0.1523099989
===> Epoch[5](3/11): Loss: 0.1447086036
===> Epoch[5](4/11): Loss: 0.1278641820
===> Epoch[5](5/11): Loss: 0.1250504106
===> Epoch[5](6/11): Loss: 0.1236538738
===> Epoch[5](7/11): Loss: 0.1182595119
===> Epoch[5](8/11): Loss: 0.0991070718
===> Epoch[5](9/11): Loss: 0.0972902104
===> Epoch[5](10/11): Loss: 0.1173191369
===> Epoch[5](11/11): Loss: 0.1108263060
|||||||||||||||||||||min_epoch_loss is 0.1229117790|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:5--Avg-PSNR: 17.4789 dB
-----max psnr model saved------
Epoch=6, lr=0.0001
===> Epoch[6](1/11): Loss: 0.1114086807
===> Epoch[6](2/11): Loss: 0.0980777070
===> Epoch[6](3/11): Loss: 0.0897672698
===> Epoch[6](4/11): Loss: 0.1172920316
===> Epoch[6](5/11): Loss: 0.1026130244
===> Epoch[6](6/11): Loss: 0.0991458148
===> Epoch[6](7/11): Loss: 0.1030992419
===> Epoch[6](8/11): Loss: 0.1193224192
===> Epoch[6](9/11): Loss: 0.1150805205
===> Epoch[6](10/11): Loss: 0.1219654530
===> Epoch[6](11/11): Loss: 0.0901976079
|||||||||||||||||||||min_epoch_loss is 0.1061790701|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:6--Avg-PSNR: 17.7218 dB
-----max psnr model saved------
Epoch=7, lr=0.0001
===> Epoch[7](1/11): Loss: 0.1040838882
===> Epoch[7](2/11): Loss: 0.1102916449
===> Epoch[7](3/11): Loss: 0.0808221325
===> Epoch[7](4/11): Loss: 0.0998512357
===> Epoch[7](5/11): Loss: 0.0970972329
===> Epoch[7](6/11): Loss: 0.0869295597
===> Epoch[7](7/11): Loss: 0.0872065350
===> Epoch[7](8/11): Loss: 0.0828588009
===> Epoch[7](9/11): Loss: 0.0821324736
===> Epoch[7](10/11): Loss: 0.0742525905
===> Epoch[7](11/11): Loss: 0.0981892198
|||||||||||||||||||||min_epoch_loss is 0.0912468467|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:7--Avg-PSNR: 18.2392 dB
-----max psnr model saved------
Epoch=8, lr=0.0001
===> Epoch[8](1/11): Loss: 0.0729542747
===> Epoch[8](2/11): Loss: 0.1028958261
===> Epoch[8](3/11): Loss: 0.0921563357
===> Epoch[8](4/11): Loss: 0.0724849924
===> Epoch[8](5/11): Loss: 0.0786597282
===> Epoch[8](6/11): Loss: 0.0783694237
===> Epoch[8](7/11): Loss: 0.0890246332
===> Epoch[8](8/11): Loss: 0.0877607912
===> Epoch[8](9/11): Loss: 0.0862702131
===> Epoch[8](10/11): Loss: 0.1024294719
===> Epoch[8](11/11): Loss: 0.0750968978
|||||||||||||||||||||min_epoch_loss is 0.0852820535|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:8--Avg-PSNR: 17.5948 dB
-----max psnr model saved------
Epoch=9, lr=0.0001
===> Epoch[9](1/11): Loss: 0.0921559483
===> Epoch[9](2/11): Loss: 0.0874731243
===> Epoch[9](3/11): Loss: 0.0928480849
===> Epoch[9](4/11): Loss: 0.0873451158
===> Epoch[9](5/11): Loss: 0.0832798183
===> Epoch[9](6/11): Loss: 0.0774093494
===> Epoch[9](7/11): Loss: 0.1091904566
===> Epoch[9](8/11): Loss: 0.0999402031
===> Epoch[9](9/11): Loss: 0.0756982937
===> Epoch[9](10/11): Loss: 0.1058755964
===> Epoch[9](11/11): Loss: 0.0858663768
epoch_avr_loss is 0.0906438516
----Start Validate----
---validate-->This-epoch:9--Avg-PSNR: 17.6502 dB
-----max psnr model saved------
Epoch=10, lr=0.0001
===> Epoch[10](1/11): Loss: 0.0812770948
===> Epoch[10](2/11): Loss: 0.0832703561
===> Epoch[10](3/11): Loss: 0.0983703956
===> Epoch[10](4/11): Loss: 0.0740538090
===> Epoch[10](5/11): Loss: 0.0767787769
===> Epoch[10](6/11): Loss: 0.0901221484
===> Epoch[10](7/11): Loss: 0.0725207478
===> Epoch[10](8/11): Loss: 0.0728225559
===> Epoch[10](9/11): Loss: 0.0869031996
===> Epoch[10](10/11): Loss: 0.0696183890
Traceback (most recent call last):
  File "train_psnr_indication.py", line 311, in <module>
    main()
  File "train_psnr_indication.py", line 157, in main
    train(training_data_loader, optimizer, model, criterion, epoch)
  File "train_psnr_indication.py", line 204, in train
    loss.backward()
  File "/home/pxk/anaconda3/envs/cxy/lib/python3.7/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/pxk/anaconda3/envs/cxy/lib/python3.7/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt