Namespace(batchSize=16, cuda=True, dataset='datasets/', lr=0.0001, momentum=0.9, nEpochs=2500, resume='', start_epoch=1, step=1000, test_path='dataset/test/', threads=16, validate_path='datasets/validate/', weight_decay=0)
===> Loading datasets
===> Building model
n_pyramids: 1 n_pyramid_cells: (3, 2, 1, 1, 1, 1)
===> Setting GPU
===> Setting Optimizer
===> Training
Epoch=1, lr=0.0001
===> Epoch[1](1/11): Loss: 0.4101140201
===> Epoch[1](2/11): Loss: 0.3765617311
===> Epoch[1](3/11): Loss: 0.3031731844
===> Epoch[1](4/11): Loss: 0.4249050617
===> Epoch[1](5/11): Loss: 0.3553695381
===> Epoch[1](6/11): Loss: 0.3379861712
===> Epoch[1](7/11): Loss: 0.3930326700
===> Epoch[1](8/11): Loss: 0.3007655740
===> Epoch[1](9/11): Loss: 0.4425457418
===> Epoch[1](10/11): Loss: 0.3255769908
===> Epoch[1](11/11): Loss: 0.4178102016
|||||||||||||||||||||min_epoch_loss is 0.3716218986|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:1--Avg-PSNR: 6.5205 dB
-----max psnr model saved------
Epoch=2, lr=0.0001
===> Epoch[2](1/11): Loss: 0.3661465645
===> Epoch[2](2/11): Loss: 0.3700370789
===> Epoch[2](3/11): Loss: 0.2361685038
===> Epoch[2](4/11): Loss: 0.3552932143
===> Epoch[2](5/11): Loss: 0.4144762456
===> Epoch[2](6/11): Loss: 0.3495522439
===> Epoch[2](7/11): Loss: 0.2989106178
===> Epoch[2](8/11): Loss: 0.3141125441
===> Epoch[2](9/11): Loss: 0.2885664105
===> Epoch[2](10/11): Loss: 0.2985075116
===> Epoch[2](11/11): Loss: 0.2885338068
|||||||||||||||||||||min_epoch_loss is 0.3254822493|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:2--Avg-PSNR: 9.3823 dB
-----max psnr model saved------
Epoch=3, lr=0.0001
===> Epoch[3](1/11): Loss: 0.2028200626
===> Epoch[3](2/11): Loss: 0.2796197534
===> Epoch[3](3/11): Loss: 0.2075506896
===> Epoch[3](4/11): Loss: 0.2163069099
===> Epoch[3](5/11): Loss: 0.2022515982
===> Epoch[3](6/11): Loss: 0.1783860624
===> Epoch[3](7/11): Loss: 0.2434241027
===> Epoch[3](8/11): Loss: 0.2086348534
===> Epoch[3](9/11): Loss: 0.2244758606
===> Epoch[3](10/11): Loss: 0.1956166476
===> Epoch[3](11/11): Loss: 0.2644098401
|||||||||||||||||||||min_epoch_loss is 0.2203178528|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:3--Avg-PSNR: 12.5828 dB
-----max psnr model saved------
Epoch=4, lr=0.0001
===> Epoch[4](1/11): Loss: 0.1524952948
===> Epoch[4](2/11): Loss: 0.1606828719
===> Epoch[4](3/11): Loss: 0.1796504259
===> Epoch[4](4/11): Loss: 0.1598608196
===> Epoch[4](5/11): Loss: 0.1492325813
===> Epoch[4](6/11): Loss: 0.1228133291
===> Epoch[4](7/11): Loss: 0.1360654682
===> Epoch[4](8/11): Loss: 0.1121844277
===> Epoch[4](9/11): Loss: 0.1231203899
===> Epoch[4](10/11): Loss: 0.1054169983
===> Epoch[4](11/11): Loss: 0.1181871593
|||||||||||||||||||||min_epoch_loss is 0.1381554333|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:4--Avg-PSNR: 16.6427 dB
-----max psnr model saved------
Epoch=5, lr=0.0001
===> Epoch[5](1/11): Loss: 0.1237861365
===> Epoch[5](2/11): Loss: 0.0931362808
===> Epoch[5](3/11): Loss: 0.1160844788
===> Epoch[5](4/11): Loss: 0.1196808070
===> Epoch[5](5/11): Loss: 0.1024559140
===> Epoch[5](6/11): Loss: 0.0953551382
===> Epoch[5](7/11): Loss: 0.1122201383
===> Epoch[5](8/11): Loss: 0.0991364345
===> Epoch[5](9/11): Loss: 0.0968522653
===> Epoch[5](10/11): Loss: 0.1196417436
===> Epoch[5](11/11): Loss: 0.1052507162
|||||||||||||||||||||min_epoch_loss is 0.1076000048|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:5--Avg-PSNR: 18.1685 dB
-----max psnr model saved------
Epoch=6, lr=0.0001
===> Epoch[6](1/11): Loss: 0.0945950970
===> Epoch[6](2/11): Loss: 0.1050571799
===> Epoch[6](3/11): Loss: 0.1298732013
===> Epoch[6](4/11): Loss: 0.1191083044
===> Epoch[6](5/11): Loss: 0.0960195810
===> Epoch[6](6/11): Loss: 0.0943663567
===> Epoch[6](7/11): Loss: 0.0887134820
===> Epoch[6](8/11): Loss: 0.0931991860
===> Epoch[6](9/11): Loss: 0.0970758051
===> Epoch[6](10/11): Loss: 0.1142597646
===> Epoch[6](11/11): Loss: 0.1110748276
|||||||||||||||||||||min_epoch_loss is 0.1039402532|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:6--Avg-PSNR: 18.1269 dB
-----max psnr model saved------
Epoch=7, lr=0.0001
===> Epoch[7](1/11): Loss: 0.1085360944
===> Epoch[7](2/11): Loss: 0.0918756425
===> Epoch[7](3/11): Loss: 0.0964827538
===> Epoch[7](4/11): Loss: 0.1044851169
===> Epoch[7](5/11): Loss: 0.1026761383
===> Epoch[7](6/11): Loss: 0.1004418880
===> Epoch[7](7/11): Loss: 0.1111546159
===> Epoch[7](8/11): Loss: 0.1127675623
===> Epoch[7](9/11): Loss: 0.1009403393
===> Epoch[7](10/11): Loss: 0.1004573703
===> Epoch[7](11/11): Loss: 0.0863403082
|||||||||||||||||||||min_epoch_loss is 0.1014688936|||||||||||||||||||||
----Start Validate----
---validate-->This-epoch:7--Avg-PSNR: 17.7146 dB
-----max psnr model saved------
Epoch=8, lr=0.0001
===> Epoch[8](1/11): Loss: 0.1122003049
===> Epoch[8](2/11): Loss: 0.0974905714
===> Epoch[8](3/11): Loss: 0.1036366150
===> Epoch[8](4/11): Loss: 0.1108282208
===> Epoch[8](5/11): Loss: 0.0827816874
===> Epoch[8](6/11): Loss: 0.0905260146
===> Epoch[8](7/11): Loss: 0.0974488407
===> Epoch[8](8/11): Loss: 0.0861247778
Traceback (most recent call last):
  File "train_psnr_indication.py", line 312, in <module>
    main()
  File "train_psnr_indication.py", line 157, in main
    train(training_data_loader, optimizer, model, criterion, epoch)
  File "train_psnr_indication.py", line 205, in train
    optimizer.step()
  File "/home/pxk/anaconda3/envs/cxy/lib/python3.7/site-packages/torch/optim/optimizer.py", line 89, in wrapper
    return func(*args, **kwargs)
  File "/home/pxk/anaconda3/envs/cxy/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/pxk/anaconda3/envs/cxy/lib/python3.7/site-packages/torch/optim/adam.py", line 119, in step
    group['eps'])
  File "/home/pxk/anaconda3/envs/cxy/lib/python3.7/site-packages/torch/optim/_functional.py", line 84, in adam
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
KeyboardInterrupt